# llm-tone-experiment

This project aims to analyze the impact of prompt tone (neutral vs. polite) on the responses generated by a Large Language Model. We explore how different manners of speech in the prompts affect various aspects of the generated responses. This analysis includes examining response word length, sentiment, embeddings, and a detailed evaluation of the responses based on specific criteria.

## Setup and installation

### Clone the Repository:

```
git clone https://github.com/MarimbaEroica/llm-tone-experiment
cd llm-tone-experiment
```

### Install Dependencies:

Use the requirements.txt file to install all necessary packages:
`pip install -r requirements.txt`

## Experiment Workflow

1. Prompt Generations
Prompts are created across 4 categories (Art & Literature, Everyday, Personal, and Technical) by querying the Langauge Model (LM) GPT-3.5. For each prompt, the LM is instructed to express the prompt in three different tones of speech: neutral (e.g. "Explain the process of X."), polite with the word please ("Please explain the process of X."), and polite without the word please ("I'm curious to know about X, would you mind explaining it to me?"). In total we generate 30 triplets of prompts per category, giving us 360 (30 * 3 * 4) unique prompts.
   
3. Response Generation:
The LM generates responses for each prompt. No addditional context is provided outside of the prompt when querying the LM for response. Responses are meticulously organized by category and prompt type.

4. Prompt Analysis:
We analyze the prompts using the following methods:

Embedding Analysis: With sentence-transformers, we compute and analyze embeddings to explore the semantic space of the prompts. We compute pairwise differences between different tones for the same prompt, and use cosine similarities to represent semantic similarities. We find significant positive similarities between these differences independent of prompt category and syntax, indicating a consistent representation for politeness in the embeddings. We explore differences between the embeddings of politeness achieved with the word "please" and without "please". We also cluster the pairwise difference embeddings between polite (with "please") and neutral prompts, and find significant heterogeneity across categories, perhaps indicating heterogeneity in the semantic effects of politeness across different prompt categories.

Sentiment Analysis: Utilizing TextBlob, we assess the sentiment (polarity) of each prompt to understand its emotional tone. We find little effect on polarity due to politeness.

5. Response Evaluation:
Responses are evaluated based on:

Word Length: We analyze the length of each response.
LM Evaluation: The LM assesses its own responses on criteria like effectiveness, politeness, depth, and conciseness. Responses are measured against the neutral versions of each prompt, regardless of which version they were responding to.
Sentiment Analysis: We perform sentiment analysis on the responses to gauge their emotional tone, measuring effects on both the subjectivity and polarity.
Embedding Analysis: Embeddings of the responses are analyzed to understand the semantic content and variation (details TBD).

## File Structure
prompts/: Stores generated prompts by prompt category.
responses/: Contains LM responses, with each unique response stored as an individual file.
analysis: Holds analysis of prompts and responses.
